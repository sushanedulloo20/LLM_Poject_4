{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"8f0609b4-8617-40e4-a35e-b2146c3abf53","_uuid":"62f2471e-52b4-4076-a0a3-8a6326fdba98","collapsed":false,"execution":{"iopub.execute_input":"2024-10-19T21:51:32.802155Z","iopub.status.busy":"2024-10-19T21:51:32.801784Z","iopub.status.idle":"2024-10-19T21:51:50.225853Z","shell.execute_reply":"2024-10-19T21:51:50.224982Z","shell.execute_reply.started":"2024-10-19T21:51:32.802122Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading model meta-llama/Llama-3.2-1B...\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Model loaded successfully.\n","Padding token set to <|end_of_text|>.\n","Model moved to GPU.\n","Base model parameters frozen.\n","Loading and tokenizing dataset...\n"]}],"source":["import os\n","import pandas as pd\n","import torch\n","from transformers import AutoTokenizer, LlamaForSequenceClassification, Trainer, TrainingArguments\n","from datasets import Dataset\n","from huggingface_hub import login\n","from sklearn.preprocessing import LabelEncoder\n","from transformers import DataCollatorWithPadding\n","from peft import get_peft_model, LoraConfig, TaskType\n","\n","TOKEN = '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'\n","login(TOKEN)\n","\n","model_name = \"meta-llama/Llama-3.2-1B\"  \n","print(f\"Loading model {model_name}...\")\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = LlamaForSequenceClassification.from_pretrained(model_name, num_labels=7)\n","print(\"Model loaded successfully.\")\n","\n","if tokenizer.pad_token is None:\n","    if tokenizer.eos_token is not None:\n","        tokenizer.pad_token = tokenizer.eos_token\n","        print(f\"Padding token set to {tokenizer.pad_token}.\")\n","    else:\n","        raise ValueError(\"Both pad_token and eos_token are None. Set a padding token.\")\n","else:\n","    print(\"Padding token already defined.\")\n","\n","model.config.pad_token_id = tokenizer.pad_token_id\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","print(f\"Model moved to {'GPU' if device.type == 'cuda' else 'CPU'}.\")\n","\n","model.gradient_checkpointing_enable()\n","\n","lora_config = LoraConfig(\n","    r=16,\n","    lora_alpha=32,\n","    lora_dropout=0.1,\n","    task_type=TaskType.SEQ_CLS,\n",")\n","\n","model = get_peft_model(model, lora_config)\n","\n","for param in model.base_model.parameters():\n","    param.requires_grad = False\n","\n","print(\"Base model parameters frozen.\")\n","\n","print(\"Loading and tokenizing dataset...\")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:51:50.362863Z","iopub.status.busy":"2024-10-19T21:51:50.362586Z","iopub.status.idle":"2024-10-19T21:51:50.446563Z","shell.execute_reply":"2024-10-19T21:51:50.445615Z","shell.execute_reply.started":"2024-10-19T21:51:50.362830Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","train_data = pd.read_json(\"/home/iiitd/LLM/dataset/meld/train.json\")\n","valid_data = pd.read_json(\"/home/iiitd/LLM/dataset/meld/valid.json\")\n","test_data = pd.read_json(\"/home/iiitd/LLM/dataset/meld/test.json\")"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:51:50.448847Z","iopub.status.busy":"2024-10-19T21:51:50.448527Z","iopub.status.idle":"2024-10-19T21:51:50.454730Z","shell.execute_reply":"2024-10-19T21:51:50.453795Z","shell.execute_reply.started":"2024-10-19T21:51:50.448812Z"},"trusted":true},"outputs":[],"source":["def aplicator(index : int, df : pd.DataFrame):\n","    t = df.iloc[max(0,index-5):index]\n","    text = \"Find the next sentiment of the given sequence:- \\n\"\n","    for ind, rw in t.iterrows():\n","        text += rw[\"input\"] + \" \" + rw[\"target\"] + '\\n'\n","    text += df[\"input\"][index] + \" \"\n","    return text"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:51:50.456038Z","iopub.status.busy":"2024-10-19T21:51:50.455737Z","iopub.status.idle":"2024-10-19T21:51:50.467261Z","shell.execute_reply":"2024-10-19T21:51:50.466425Z","shell.execute_reply.started":"2024-10-19T21:51:50.456006Z"},"trusted":true},"outputs":[],"source":["def null_aplicator(index: int, df: pd.DataFrame):\n","    if((index+1) % 5 != 0):\n","        return None\n","    return aplicator(index,df)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:51:50.468888Z","iopub.status.busy":"2024-10-19T21:51:50.468440Z","iopub.status.idle":"2024-10-19T21:51:55.793037Z","shell.execute_reply":"2024-10-19T21:51:55.791898Z","shell.execute_reply.started":"2024-10-19T21:51:50.468846Z"},"trusted":true},"outputs":[],"source":["train_data[\"comb\"] = train_data.apply(lambda x: aplicator(int(x.name),train_data),axis=1)\n","valid_data[\"comb\"] = valid_data.apply(lambda x: aplicator(int(x.name),valid_data),axis=1)\n","test_data[\"comb\"] = test_data.apply(lambda x: aplicator(int(x.name),test_data),axis=1)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:51:55.795160Z","iopub.status.busy":"2024-10-19T21:51:55.794627Z","iopub.status.idle":"2024-10-19T21:51:56.998364Z","shell.execute_reply":"2024-10-19T21:51:56.997267Z","shell.execute_reply.started":"2024-10-19T21:51:55.795106Z"},"trusted":true},"outputs":[],"source":["train_data[\"ncomb\"] = train_data.apply(lambda x: null_aplicator(int(x.name),train_data),axis=1)\n","valid_data[\"ncomb\"] = valid_data.apply(lambda x: null_aplicator(int(x.name),valid_data),axis=1)\n","test_data[\"ncomb\"] = test_data.apply(lambda x: null_aplicator(int(x.name),test_data),axis=1)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:52:10.632790Z","iopub.status.busy":"2024-10-19T21:52:10.632396Z","iopub.status.idle":"2024-10-19T21:52:10.651273Z","shell.execute_reply":"2024-10-19T21:52:10.650368Z","shell.execute_reply.started":"2024-10-19T21:52:10.632753Z"},"trusted":true},"outputs":[],"source":["train_data_combined = train_data.dropna()\n","valid_data_combined = valid_data.dropna()\n","test_data_combined = test_data.dropna()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:54:14.024666Z","iopub.status.busy":"2024-10-19T21:54:14.023922Z","iopub.status.idle":"2024-10-19T21:56:56.515796Z","shell.execute_reply":"2024-10-19T21:56:56.511358Z","shell.execute_reply.started":"2024-10-19T21:54:14.024608Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenizing datasets...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"40300b2ec23b476983d3eb12844a907e","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/9989 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"62400923835d424a9d2e475bd3dd9f2d","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1109 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c10b290b92d2446d9d968d95c59315ac","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2610 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Datasets tokenized successfully.\n","Training arguments set up successfully.\n","Trainer created successfully. Starting training...\n"]},{"name":"stderr","output_type":"stream","text":["`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","/home/iiitd/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1500/1500 31:43, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>4.336400</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>4.337200</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>4.336500</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/iiitd/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","/home/iiitd/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Training complete.\n"]}],"source":["import numpy as np\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","label_encoder = LabelEncoder()\n","label_encoder.fit(train_data['target'])\n","\n","train_data['label'] = label_encoder.transform(train_data['target'])\n","valid_data['label'] = label_encoder.transform(valid_data['target'])\n","test_data['label'] = label_encoder.transform(test_data['target'])\n","\n","train_dataset = Dataset.from_pandas(train_data)\n","valid_dataset = Dataset.from_pandas(valid_data)\n","test_dataset = Dataset.from_pandas(test_data)\n","\n","def preprocess_function(examples):\n","    tokenized_output = tokenizer(\n","        examples['comb'], \n","        truncation=True, \n","        padding='longest',\n","        return_tensors='pt'\n","    )\n","    tokenized_output['label'] = examples['label']\n","    return tokenized_output\n","\n","print(\"Tokenizing datasets...\")\n","tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n","tokenized_valid_dataset = valid_dataset.map(preprocess_function, batched=True)\n","tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n","print(\"Datasets tokenized successfully.\")\n","\n","tokenized_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n","tokenized_valid_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n","tokenized_test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    acc = accuracy_score(labels, predictions)\n","    f1_weighted = f1_score(labels, predictions, average='weighted')\n","    f1_macro = f1_score(labels, predictions, average='macro')\n","    return {\n","        'accuracy': acc,\n","        'f1_weighted': f1_weighted,\n","        'f1_macro': f1_macro\n","    }\n","\n","\n","training_args = TrainingArguments(\n","    output_dir='./Adaptive-Finetuned-Llama',\n","    per_device_train_batch_size=10,\n","    per_device_eval_batch_size=10,\n","    num_train_epochs=3,\n","    logging_dir='./logs',\n","    no_cuda=False,\n","    fp16=True,\n",")\n","\n","print(\"Training arguments set up successfully.\")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train_dataset,\n","    eval_dataset=tokenized_valid_dataset,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics\n",")\n","\n","print(\"Trainer created successfully. Starting training...\")\n","trainer.train()\n","print(\"Training complete.\")"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["test_results = trainer.evaluate(tokenized_test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_results"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["eval_loss : 4.34068489074707\n","eval_accuracy : 0.05938697318007663\n","eval_f1_weighted : 0.015273100284119514\n","eval_f1_macro : 0.03061197000869658\n","eval_runtime : 79.7674\n","eval_samples_per_second : 32.72\n","eval_steps_per_second : 1.642\n","epoch : 3.0\n"]}],"source":["for i,j in test_results.items():\n","  print(f'{i} : {j}')"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model and tokenizer saved.\n","Training completed successfully.\n"]}],"source":["model.save_pretrained('./Adaptive-Finetuned-Llama/model')\n","tokenizer.save_pretrained('./Adaptive-Finetuned-Llama/tokenizer')\n","print(\"Model and tokenizer saved.\")\n","print(\"Training completed successfully.\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5909309,"sourceId":9670188,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"lllm","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
